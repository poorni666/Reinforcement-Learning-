# -*- coding: utf-8 -*-
"""wordle_env.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YuIBB2OhbzPEwgjvybAp4IDLqy39vbjv
"""

from google.colab import drive
drive.mount('/content/drive')

import numpy as np
import torch
import random
from collections import Counter
from wordle_core import wordle_feedback, is_consistent

class WordleEnv:
    """
    - Rewards shaped to encourage efficiency and information gain
    - Training mask only blocks repeated guesses
    """

    def __init__(self, vocab, solutions, word_to_idx, max_turns=6, training_mode=True):
        self.vocab = vocab
        self.solutions = solutions
        self.word_to_idx = word_to_idx
        self.vocab_size = len(vocab)
        self.max_turns = max_turns
        self.training_mode = training_mode
        self.reset()

    def reset(self, target=None):
        self.target = target or random.choice(self.solutions)
        self.history = []
        self.turn = 0
        self.done = False
        self.best_info = 0
        return self._get_obs(), self._get_mask()

    def _get_obs(self):
        obs = np.zeros(725, dtype=np.float32)
        for t, (guess, fb) in enumerate(self.history[-5:]):
            offset = t * 145
            for i, ch in enumerate(guess):
                if 'a' <= ch <= 'z':
                    obs[offset + i*26 + (ord(ch) - ord('a'))] = 1.0
            for i, v in enumerate(fb):
                obs[offset + 130 + i*3 + v] = 1.0
        return torch.tensor(obs, dtype=torch.float32)

    def _get_mask(self):
        """masking , only prevent repeated guesses during training"""
        if self.training_mode:
            guessed_words = {g for g, _ in self.history}
            mask = np.ones(self.vocab_size, dtype=np.float32)
            for word in guessed_words:
                if word in self.word_to_idx:
                    mask[self.word_to_idx[word]] = 0.0
            return torch.tensor(mask, dtype=torch.float32)
        else:
            # Evaluation: enforce Wordle rules for fair comparison
            consistent = [w for w in self.vocab if is_consistent(self.history, w)]
            guessed_words = {g for g, _ in self.history}
            consistent = [w for w in consistent if w not in guessed_words]
            mask = np.zeros(self.vocab_size, dtype=np.float32)
            for w in consistent:
                mask[self.word_to_idx[w]] = 1.0
            return torch.tensor(mask, dtype=torch.float32)

    def step(self, action):
        if isinstance(action, torch.Tensor):
            action = action.item()

        guess = self.vocab[action]
        self.turn += 1
        fb = wordle_feedback(guess, self.target)
        self.history.append((guess, fb))

        reward = 0.0

        # REWARD DESIGN
        if guess == self.target:
            # Strong bonus for solving early to encourage efficiency
            reward = 10.0 + (self.max_turns - self.turn) * 2.0
            self.done = True
        elif self.turn >= self.max_turns:
            reward = -5.0
            self.done = True
        else:
            green_letters = sum(1 for f in fb if f == 2)
            yellow_letters = sum(1 for f in fb if f == 1)

            # Reward feedback quality
            reward += green_letters * 0.8
            reward += yellow_letters * 0.3

            # Penalize uninformative guesses (all gray)
            if green_letters == 0 and yellow_letters == 0:
                reward -= 0.5

        obs = self._get_obs()
        mask = self._get_mask()
        return obs, reward, self.done, mask